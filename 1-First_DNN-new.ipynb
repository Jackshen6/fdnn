{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "1-First_DNN-new.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVbCo1YpguEC"
      },
      "source": [
        "# First Deep Neural Network (DNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfiHPthNguEC"
      },
      "source": [
        "This notebook follows along with the lecture on your first DNN as well as the advanced topics lecture.\n",
        "\n",
        "It uses a very simple data set and model. The data is a Pima Indians onset of diabetes dataset. It has a\n",
        "number of factors and tests as input variables with a simple binary classification as output\n",
        "(1 = onset of diabetes, 0 = no onset of diabetes). It is originally from the National Institute of\n",
        "Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically\n",
        "predict whether or not a patient has the on-set of diabetes, based on certain diagnostic measurements\n",
        "included in the dataset.\n",
        "\n",
        "The data is in csv format at the following URL: \n",
        "\n",
        "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\n",
        "\n",
        "This lecture will be using a very simple 3 layer Fully Connected Network (FCN) as our model.\n",
        "We'll walk through the steps of training a NN using <a href=\"https://www.tensorflow.org/\">Tensorflow</a>\n",
        "from <a href=\"https://www.anaconda.com/\">Anaconda</a>. For building and training models,\n",
        "<a href=\"https://www.tensorflow.org/guide/keras\">Tensorflow.keras</a> will be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb0aq7IIguEC"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUNnLCeTguEC"
      },
      "source": [
        "# Checking Tensorflow and Keras vesion and getting matplotlib ready\n",
        "\n",
        "The first step is to determine the installed versions of Tensorflow and tensorflow.keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5_qbdcaguEC"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il6ovG75guEC"
      },
      "source": [
        "print(tf.keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFh-h0m4guEC"
      },
      "source": [
        "For graphs and visualization, we'll start out using <a href=\"https://matplotlib.org/\">matplotlib</a>.\n",
        "Let's set up matplotlib to plot inside the notebook using a Jupyter \"magic\" command.\n",
        "\n",
        "For some silly reason we need to run this twice (4/6/2020). You will see an error when running\n",
        "the first cell. Don't worry about it - just run the second cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37CwR8Z3guEC"
      },
      "source": [
        "%maplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uvw-rJWCguED"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yToMjgsHheUc"
      },
      "source": [
        "Now let's upload the dataset for our model.\n",
        "\n",
        "https://raw.githubusercontent.com/keipertk/fdnn/master/pima-indians-diabetes.data.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNEhsrlehkNH"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyDe4NuzguED"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUg3KFNFguED"
      },
      "source": [
        "## First Model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E4RmDVlguED"
      },
      "source": [
        "The lecture covers seven steps of training a NN model. It also walks through\n",
        "the development of the Python code using Keras to perform the training.\n",
        "The final code is presented below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i4NmAnmguED"
      },
      "source": [
        "# Create first network with Keras\n",
        "\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X, Y, epochs=150, batch_size=10,  verbose=1)\n",
        "\n",
        "# calculate predictions\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# round predictions\n",
        "rounded = [round(x[0]) for x in predictions]\n",
        "print(rounded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJdchAVCguED"
      },
      "source": [
        "There is a simple command to get an ASCII summary of the model. Run the\n",
        "command below to see the output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-zElMWJguED"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUQchbPxguED"
      },
      "source": [
        "When the training is done, the results are stored in the dictionary \"history\". The command below lists the keys in the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL3qlcl6guED"
      },
      "source": [
        "print(history.history.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DetOKExjguED"
      },
      "source": [
        "Next, let's plot the training history of accuracy (\"accuracy\") and the loss function (\"loss\"). First, let's plot the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB7MwkVNguED"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFHinw_KguED"
      },
      "source": [
        "Next let's plot the loss as a function of the epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6ZeGowTguED"
      },
      "source": [
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnLbBYSgguED"
      },
      "source": [
        "## End of First Lecture\n",
        "\n",
        "This marks the end of the first lecture. The next part of this notebook goes along with the second lecture on more \"advanced\" topics with DNN's and Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW4ShRvWguED"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR1hOVQWguED"
      },
      "source": [
        "## What is the Keras Ouput telling us? And how can we get more performance timings?\n",
        "\n",
        "I usually run Keras with the option \"verbose=1\" to get a little more information. The output from Keras is\n",
        "fairly compact but provides some good information.\n",
        "\n",
        "The code below is the \"final\" code used in the previous lecture. Go ahead and run it, paying attention to the\n",
        "output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6CoT5Z_guED"
      },
      "source": [
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\n",
        "# Model.summary\n",
        "print(\"Model Summary:\")\n",
        "model.summary()\n",
        "print(\" \")\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X, Y, epochs=150, batch_size=10,  verbose=1)\n",
        "\n",
        "# calculate predictions\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Plots\n",
        "plt.plot(history.history['acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aLaOCouguED"
      },
      "source": [
        "A sample output line for an epoch looks like the following:\n",
        "    \n",
        "    \n",
        "\n",
        "Epoch 7/150\n",
        "768/768 [==============================] - 0s 366us/step - loss: 0.6190 - accuracy: 0.6510 \n",
        "\n",
        "\n",
        "\n",
        "The first line tells which epoch is being process and the total number of epochs.\n",
        "\n",
        "The second line gives you a progress bar so you know how the epoch is processing. This is followed by\n",
        "the amount of time spent doing the training for a \"step\" in that epoch. In the above case, it took 0 seconds\n",
        "(it was actually under 1 second and the output can only print in seconds increments). This is followed\n",
        "by the average time per input catch. For this example is was 366 microseconds (366 us/step).\n",
        "\n",
        "Finally, <tt>tf.keras</tt> outputs the the value of the loss function for that epoch as well as the\n",
        "accuracy.\n",
        "\n",
        "This can give some useful information about the model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLFsFAU6guED"
      },
      "source": [
        "## Total training time\n",
        "\n",
        "To get total training time, simply use the Python “time” module. The first thing is\n",
        "to call the time before <tt>model.fit()</tt> method is called. Then you call the time\n",
        "after <tt>model.fit()</tt>. The difference in time is the time it took to train the\n",
        "model. The code below adds this timing to our code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McabDNkCguED"
      },
      "source": [
        "# Create first network with Keras\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\n",
        "# Model.summary\n",
        "print(\"Model Summary:\")\n",
        "model.summary()\n",
        "print(\" \")\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "start_time = time.time()\n",
        "history = model.fit(X, Y, validation_split=0.20, epochs=150, batch_size=10, verbose=1)\n",
        "end_time = time.time()\n",
        "\n",
        "# Plots\n",
        "plt.plot(history.history['acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# total training time\n",
        "print(\" \")\n",
        "print(\"Total training time = \",(end_time - start_time),\" seconds\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwE8x72-guED"
      },
      "source": [
        "## Times for each epoch.\n",
        "\n",
        "Another metric we might be interested in is how can you easily capture the time for each epoch?\n",
        "\n",
        "tf.keras has the concept of “callback” function(s)\n",
        "(https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback). You can use\n",
        "callbacks to get a view on internal states and statistics of the model during training. You\n",
        "pass a list of callback functions to the <tt>fit</tt> method of the <tt>sequential</tt> or <tt>model</tt>\n",
        "classes. The relevant methods of the callbacks are called at each stage of the training.\n",
        "We will use the example of a callback in the previous link. It will use the Python time module\n",
        "to capture epoch times.\n",
        "\n",
        "Fortunately, Stack Overflow has an elegant solution.\n",
        "\n",
        "https://stackoverflow.com/questions/43178668/record-the-computation-time-for-each-epoch-in-keras-during-model-fit#43186440\n",
        "\n",
        "that can be adapted to tensorflow.keras.\n",
        "\n",
        "The Stack Overflow code uses the same approach as we used for total training time. We call <tt>time()</tt>\n",
        "before epoch and call <tt>time()</tt> after epoch. The elapsed time is the difference in times. The elapsed\n",
        "time is stored in the callback object (it is in seconds). The we pass the list of callback\n",
        "functions to <tt>model.fit()</tt>. Then you can use the epoch time results as you see fit.\n",
        "\n",
        "The code below computes statistics on the epoch times. It also plots the epoch times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OalmxblSguED"
      },
      "source": [
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import statistics\n",
        "\n",
        "# callback class to store epoch times\n",
        "class TimeHistory(tf.keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.times = []\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.epoch_time_start = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.times.append(time.time() - self.epoch_time_start)\n",
        "# end class\n",
        "\n",
        "        \n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Create callback for epoch times\n",
        "time_callback = TimeHistory()\n",
        "callbacks_list = [time_callback]\n",
        "\n",
        "# Fit the model\n",
        "start_time = time.time()\n",
        "history = model.fit(X, Y, validation_split=0.20, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=1)\n",
        "end_time = time.time()\n",
        "\n",
        "# total training time\n",
        "print(\" \")\n",
        "print(\"Total training time = \",(end_time - start_time),\" seconds\")\n",
        "print(\" \")\n",
        "\n",
        "# Print out epoch timeings\n",
        "epoch_times = time_callback.times\n",
        "min_time = min(epoch_times)\n",
        "max_time = max(epoch_times)\n",
        "mean_time = statistics.mean(epoch_times)\n",
        "median_time = statistics.median(epoch_times)\n",
        "stdev_time = statistics.stdev(epoch_times)\n",
        "variance_time = statistics.variance(epoch_times)\n",
        "\n",
        "print(\"min epoch time = \",min_time,\" seconds\")\n",
        "print(\"max epoch time = \",max_time,\" seconds\")\n",
        "print(\"Mean epoch time = \",mean_time,\" seconds\")\n",
        "print(\"Median epoch time = \",median_time,\" seconds\")\n",
        "print(\"Standard deviation epoch time = \",stdev_time,\" seconds\")\n",
        "print(\"Variance epoch time = \",variance_time,\" seconds\")\n",
        "print(\" \")\n",
        "\n",
        "plt.plot(epoch_times)\n",
        "plt.title('epoch times')\n",
        "plt.ylabel('time')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXxxkEDAguED"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qdEGtnkguED"
      },
      "source": [
        "## Visualizing the Model\n",
        "\n",
        "Getting a print out of the model using, <tt>model.summary()</tt>,\n",
        "works well enough. However, when the models get complex, it would be good to \n",
        "get a better image of the model. There is a function in <tt>tf.keras</tt> that\n",
        "can do this for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "103qutIQguED"
      },
      "source": [
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pydot\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\n",
        "# Model.summary\n",
        "print(\"Model Summary:\")\n",
        "model.summary()\n",
        "print(\" \")\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X, Y, validation_split=0.20, epochs=150, batch_size=10, verbose=1)\n",
        "\n",
        "# calculate predictions\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# round predictions\n",
        "#rounded = [round(x[0]) for x in predictions]\n",
        "#print(rounded)\n",
        "\n",
        "tf.keras.utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# Need to show image here\n",
        "from IPython.display import Image\n",
        "Image(\"model_plot.png\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynBzv2JiguED"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vktFOvp0guED"
      },
      "source": [
        "## Split data into train and test\n",
        "\n",
        "In the previous training, the entire dataset was used for the training. In general,\n",
        "we want a portion of the dataset to be used for testing as the model is trained.\n",
        "This gives us a better view of the accuracy and loss of the training process because\n",
        "the data is not used in the training. Typically we call this \"validation\" data. \n",
        "\n",
        "<tt>tf.keras</tt> allows us to specify how much of the dataset should be set aside for validation.\n",
        "It is expressed as a decimal fraction between 0.0 and 0.99 (using 1.0 means the model\n",
        "would never be trained)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UKKqz2sguED"
      },
      "source": [
        "import numpy\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X, Y, validation_split=0.20, epochs=150, batch_size=10, verbose=1)\n",
        "\n",
        "# calculate predictions\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# round predictions\n",
        "#rounded = [round(x[0]) for x in predictions]\n",
        "#print(rounded)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIE32MBIguED"
      },
      "source": [
        "As with the previous example, let's examine what is in the \"history\" dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IqGDtUdguED"
      },
      "source": [
        "print(history.history.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_x9x75tguED"
      },
      "source": [
        "Notice that in addition to the training results and metrics, we have the validation results and metrics. These are designated with the \"val_\" as the first part of the name.\n",
        "\n",
        "Let's plot both the training and validation results for the accuracy as a function of the epoch. The first plot is the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo6htp_6guED"
      },
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O655oRAJguED"
      },
      "source": [
        "Let's plot the loss function for both the training data andthe validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zqe-e0LWguED"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nqr-DBPguED"
      },
      "source": [
        "## Create a Checkpoint - weights only, every epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCI0n1R8guED"
      },
      "source": [
        "The next few sections discuss how to create a checkpoint of the weights and/or model during training.\n",
        "Sometimes training can take a very long time so it's a good idea to create a checkpoint of the model\n",
        "periodically. A checkpoint is just the state of the model training at some number of epochs. You can\n",
        "checkpoint just the weights or the entire model, weights, and the training parameters (loss function,\n",
        " optimizer, etc.).\n",
        "\n",
        "To do this, you use a <tt>callback</tt> function that tf.keras provides,\n",
        "<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\">ModelCheckpoint</a>.\n",
        "It is much simpler than it sounds. You just provide a few inputs including a file name and tf.keras\n",
        "takes care of the rest.\n",
        "\n",
        "There are lots of options for creating a check-point. To get started, this first example below creates a\n",
        "checkpoint at the end of each epoch. Even though this problem is tiny, the number of epochs has been\n",
        "changed to 10 so you don't end up with a large number of checkpoint files.\n",
        "\n",
        "The first step in the code is to define the \"filepath\" (this is the filename of the output file). Then you\n",
        "use the tf.keras function <tt>ModelCheckpoint</tt> to define the other variables. For this example,\n",
        "only the weights are saved. The\n",
        "<tt>verbosity</tt> is set to 1 (you may get a fair amount of output but this preferred to not seeing any\n",
        "or extremely limited output). Also the option <tt>save_freq</tt> is set to 1 which tells\n",
        "<tt>tf.keras</tt> to create the checkpoint every epoch. You can change the value of \"1\" to any integer\n",
        "you like. For example, setting to 2 would mean that the weights are saved every other epoch. You can also\n",
        "set it to <tt>\"epoch\"</tt> that saves the output after every epoch.\n",
        "\n",
        "The code for the model checkpoint we'll be using looks like the following.\n",
        "<code>\n",
        "filepath=\"weights-improvement-{epoch:02d}-{accuracy:.2f}.hdf5\"\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, verbose=1, save_freq='epoch')\n",
        "callbacks_list = [checkpoint]\n",
        "</code>\n",
        "\n",
        "\n",
        "For <tt>filepath</tt> we have created a file name that includes the epoch number and the <tt>accuracy</tt>.\n",
        "An example of a file name might be \"weights-improvement-08-0.89.hdf5\" where \"08\" is the epoch number and\n",
        "\"0.89\" is the accuracy. Note - the code is using hdf5 as file format.\n",
        "\n",
        "The second line of code creates a Checkpoint object with a verbosity of 1 so we can see what the\n",
        "code is doing. It also is only saving the model weights (default) and the period in between saving the\n",
        "weights is every epoch. For a larger number of epochs you could set the period to something larger,\n",
        "perhaps 10 or 100.\n",
        "\n",
        "The third line just adds the checkpoint object to a list of callback functions that are passed to\n",
        "<tt>tf.keras</tt>. You can provide a list of callback functions if you want.\n",
        "\n",
        "In this example, it should create 50 files (50 epochs) in the directory where this Jupyter\n",
        "notebook is located. After running the cell take a look at the directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d35KzlZQguED"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, kernel_initializer='uniform', activation='sigmoid'))      \n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define checkpoint variables\n",
        "filepath=\"weights-improvement-{epoch:02d}-{accuracy:.2f}.hdf5\"\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, verbose=1, save_freq='epoch')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X, Y, validation_split=0.20, epochs=50, batch_size=10,\n",
        "                    callbacks=callbacks_list, verbose=1)\n",
        "\n",
        "# calculate predictions\n",
        "predictions = model.predict(X)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNQrDSiIguED"
      },
      "source": [
        "List all files in current directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORAtgujJguED"
      },
      "source": [
        "# https://realpython.com/working-with-files-in-python/\n",
        "import os\n",
        "\n",
        "# List all files in a directory using scandir()\n",
        "basepath = '.'\n",
        "with os.scandir(basepath) as entries:\n",
        "    for entry in entries:\n",
        "        if entry.is_file():\n",
        "            print(entry.name)\n",
        "        # end if\n",
        "    # end for\n",
        "# end with"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKvv2XlwguEE"
      },
      "source": [
        "## Check point - only save weights when there is improvement in accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjexE3kgguEE"
      },
      "source": [
        "Next, let us checkpoint the weights but only when there is improvement in some important\n",
        "metric. These are typically one of the metrics we watch to understand how the training is\n",
        "progressing - loss and accuracy. For this example, we will use \"accuracy\".\n",
        "\n",
        "Using this metric, we can create a <tt>ModelCheckpoint</tt> function to save the weights.\n",
        "Using the options, we can save the weights when <em>there is improvement in accuracy</em>.\n",
        "The option we use is, <tt>save_best_only=True</tt>. The code for this looks like the following:\n",
        "                                                                         \n",
        "                                                                         \n",
        "filepath=\"weights-improvement-{epoch:02d}-{accuracy:.2f}.hdf5\"\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, verbose=1, save_weights_only=True,\n",
        "                                                monitor=\"accuracy\", save_best_only=True)\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "\n",
        "Relative to the last example, we're using the option <tt>save_weights_only=True</tt> and to\n",
        "monitor the \"accuracy\" of the training. Otherwise, everything is the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuZ8tA3WguEE"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define checkpoint variables\n",
        "filepath=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, verbose=1, save_weights_only=True,\n",
        "                                                monitor=\"accuracy\", save_best_only=True)\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X, Y, validation_split=0.20, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)\n",
        "\n",
        "# calculate predictions\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# round predictions\n",
        "#rounded = [round(x[0]) for x in predictions]\n",
        "#print(rounded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm6kHdXTguEE"
      },
      "source": [
        "You won't see a file for every epoch, only those where the training accuracy improves.\n",
        "\n",
        "The files are listed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Wn0P9jZguEE"
      },
      "source": [
        "# https://realpython.com/working-with-files-in-python/\n",
        "import os\n",
        "\n",
        "# List all files in a directory using scandir()\n",
        "basepath = '.'\n",
        "with os.scandir(basepath) as entries:\n",
        "    for entry in entries:\n",
        "        if entry.is_file():\n",
        "            print(entry.name)\n",
        "        # end if\n",
        "    # end for\n",
        "# end with"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96QqTLcNguEE"
      },
      "source": [
        "## Checkpoint - save best weights during training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIt1kbrUguEE"
      },
      "source": [
        "The next example allows us to save the weights for epoch with the highest accuracy\n",
        "as the training progresses. There will be only one file that is over-written every\n",
        "time the accuracy is improved. We only need one file for this so we don't need to\n",
        "worry about the number of epochs were the improvement occurs or the accuracy.\n",
        "This is the only change we need to make."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx-lw2FoguEE"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# estimate accuracy on whole dataset using loaded weights\n",
        "scores = model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "# create model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define checkpoint variables\n",
        "filepath=\"weights-best.hdf5\"\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, verbose=1, save_weights_only=True,\n",
        "                                                monitor=\"accuracy\", save_best_only=True)\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X, Y, validation_split=0.20, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)\n",
        "\n",
        "# calculate predictions\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# round predictions\n",
        "#rounded = [round(x[0]) for x in predictions]\n",
        "#print(rounded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A5TqjEqguEE"
      },
      "source": [
        "## Load checkpoint and evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC588ClAguEE"
      },
      "source": [
        "We can now check-point the training of our model based on various conditions. It would\n",
        "be really nice to be to be able to reuse the saved weights or model in some fashion.\n",
        "For example, this would allow you to examine the model and the training history or perhaps\n",
        "restart the training if you want (perhaps you can change the training parameters to\n",
        "increase the training rate or improve the accuracy, etc.).\n",
        "\n",
        "This first example is simple - it just reloads the save weights from the last example,\n",
        "which save the \"best\" weights, corresponding the best value of accuracy, and evaluates\n",
        "this model for these weights.\n",
        "\n",
        "There are a couple of steps we need to take to our code to reload the saved model weights.\n",
        "The first one is that we need to re-create the model that corresponds to the weights since\n",
        "we didn't save the entire model. An easy way to do this is to simply just re-define the\n",
        "model. The code for this is below.\n",
        "\n",
        "\n",
        "\n",
        "*model = tf.keras.models.Sequential()*\n",
        "*model.add(tf.keras.layers.Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))*\n",
        "*model.add(tf.keras.layers.Dense(8, kernel_initializer='uniform', activation='relu'))*\n",
        "*model.add(tf.keras.layers.Dense(1, kernel_initializer='uniform', activation='sigmoid'))*\n",
        "\n",
        "\n",
        "\n",
        "Next, we load the weights using the <tt>model.load_weights</tt> function where \"model\" is our\n",
        "recreated model. The code for this is also simple.\n",
        "\n",
        "\n",
        "*model.load_weights(\"weights-best.hdf5\")*\n",
        "\n",
        "\n",
        "Next we \"compile\" the model and then load the \"dataset\" or the original input file.\n",
        "Note - you can load any data set you want as along as it is in the same \"form\" as the\n",
        "original data set. As an aside this would be an opportunity to load data that the model\n",
        "has never seen so you can evaluation its performance.\n",
        "\n",
        "In this example, after the model is redefined and the weights are loaded, we \"evaluate\" the\n",
        "model with the saved weights. For this we use the function <tt>mode.evaluate</tt>. Then we\n",
        "print the \"score\" for the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6gp0QuSguEE"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# create model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\n",
        "# load weights\n",
        "model.load_weights(\"weights-best.hdf5\")\n",
        "\n",
        "# Compile model (required to make predictions)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(\"Created model and loaded weights from file\")\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# estimate accuracy on whole dataset using loaded weights\n",
        "scores = model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WmhPwrTguEE"
      },
      "source": [
        "## Reload check point and continue training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3e-a7qEguEE"
      },
      "source": [
        "The last example just \"evaluated\" the \"best\" model weights once they were loaded.\n",
        "No further training was done. But what if we <strong>wanted</strong> to continue\n",
        "training the model. How would we do this?\n",
        "\n",
        "The process is very similar to the last example. We first recreate the model and\n",
        "load the original input data. We next compile the model with the training parameters\n",
        "we want to use. These can be different than the ones used in the previous training(s).\n",
        "Do not forget to redefine the <tt>ModelCheckpoint</tt> function because we may want to\n",
        "check-point the model during the training.\n",
        "\n",
        "Finally we call the function <tt>model.fit</tt> to start the training. For this case\n",
        "a total 150 epochs are used.\n",
        "\n",
        "For this particular example, at the end of training, the various entries in the\n",
        "input data set are evaluated and their corresponding \"score\", or <strong>accuracy</strong>\n",
        "in this case, are printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCyC2CnXguEE"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# estimate accuracy on whole dataset using loaded weights\n",
        "scores = model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "# create model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\n",
        "# load weights\n",
        "model.load_weights(\"weights-best.hdf5\")\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define checkpoint variables\n",
        "filepath=\"weights-best-new.hdf5\"\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, verbose=1, monitor=\"val_accuracy\", \n",
        "                                                save_best_only=True, mode=\"max\")\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X, Y, validation_split=0.20, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=1)\n",
        "\n",
        "# calculate predictions\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# round predictions\n",
        "#rounded = [round(x[0]) for x in predictions]\n",
        "#print(rounded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rNIEV4LguEE"
      },
      "source": [
        "Notice that the code saves the best weights as before. We're writing to the same\n",
        "file over-writing the previous weights. If you want to save the previous weights make\n",
        "a copy before running the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkLCWT07guEE"
      },
      "source": [
        "# New TF Functions - tf.keras.models.save_model\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model\n",
        "\n",
        "Saves a model as a TensorFlow SavedModel or HDF5 file.\n",
        "\n",
        "\n",
        "tf.keras.models.save_model(\n",
        "    model, filepath, overwrite=True, include_optimizer=True, save_format=None,\n",
        "    signatures=None, options=None\n",
        ")\n",
        "\n",
        "The saved model contains:\n",
        "\n",
        "- the model's configuration (topology)\n",
        "- the model's weights\n",
        "- the model's optimizer's state (if any)\n",
        "\n",
        "Thus the saved model can be reinstantiated in the exact same state, without\n",
        "any of the code used for model definition or training.\n",
        "\n",
        "\n",
        "https://www.tensorflow.org/tutorials/keras/save_and_load\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbcQtM-aguEE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJNyvPWJguEE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRt2eGlcguEE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfbEpuVSguEE"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_tVQlhUguEE"
      },
      "source": [
        "## Visualizing Accuracy in real-time\n",
        "\n",
        "Creating plots of the training history is nice, but so far we can only do that\n",
        "<em>after</em> the training is completed. Plotting the \"history\" is always very\n",
        "useful but there is nothing like seeing the training history to date as the\n",
        "training progresses.\n",
        "\n",
        "\n",
        "\n",
        "There is a Python module that can integrate with <tt>tf.keras</tt> that can do\n",
        "real-time plotting. <a href=\"https://github.com/stared/livelossplot/\">Linelossplot</a>\n",
        "is a module that has functions that can do this for you. At this time (April 2020)\n",
        "Anaconda cannot install it, but <tt>pip</tt> can. I would not worry too much about\n",
        "mixing Anaconda and pip.\n",
        "\n",
        "In the example below, we will use livelossplot for our simple example with 150 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ob3A8-aguEE"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from livelossplot.keras import PlotLossesCallback\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\n",
        "callbacks_list = [PlotLossesCallback()]\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X, Y, validation_split=0.20, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6n4IuO7guEE"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK752B9ZguEE"
      },
      "source": [
        "# Work in Progress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_sageojguEE"
      },
      "source": [
        "## Visualize Activations with keract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyENPzmAguEE"
      },
      "source": [
        "$ pip install keract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUWAbPMwguEE"
      },
      "source": [
        "# Create first network with Keras\n",
        "\n",
        "#model = tf.keras.models.Sequential()\n",
        "#model.add(tf.keras.layers.Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "#model.add(tf.keras.layers.Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "#model.add(tf.keras.layers.Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "#checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, verbose=1, save_weights_only=True,\n",
        "#                                               monitor=\"val_accuracy\", save_best_only=True, mode=\"max\")\n",
        "import tensorflow as tf\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from keract import get_activations\n",
        "\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X, Y, validation_split=0.20, epochs=150, batch_size=10, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv3M0K7_guEE"
      },
      "source": [
        "from keract import get_activations\n",
        "\n",
        "activations = get_activations(model, x, layer_name)\n",
        "#keract.get_activations(model, x, layer_name=None, nodes_to_evaluate=None, output_format='simple',\n",
        "#                       auto_compile=True)\n",
        "\n",
        "#where model is the model object, x is the input (numpy array), and layer_name is the layer to get activations\n",
        "#for if youonly want the activations for one layer (it is optional)\n",
        "\n",
        "activation - a dictionary mapping layers to their activations (the output of get_activations)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMlmjImZguEE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWCJzQmJguEE"
      },
      "source": [
        "## Pattern for Defining Layers\n",
        "\n",
        "So far the code has defined each layer with one line of code. This works extremely well and keeps the code compact but it can make understanding the model a bit more difficult. We've been dealing with Perceptron models so far so understanding them isn't too difficult, but for more complicated networks it would be good to build the model with clarity in mind.\n",
        "\n",
        "One possibility for improving clarity is to build each layer in the model with several statements. As an example, let's start with our existing model definition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB0dy4VuguEE"
      },
      "source": [
        "# Create first network with Keras\n",
        "\n",
        "#model = tf.keras.models.Sequential()\n",
        "#model.add(tf.keras.layers.Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "#model.add(tf.keras.layers.Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "#model.add(tf.keras.layers.Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "#checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, verbose=1, save_weights_only=True,\n",
        "#                                               monitor=\"val_accuracy\", save_best_only=True, mode=\"max\")\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# create model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt3y4jQdguEE"
      },
      "source": [
        "Note: You may have to run the cell twice to get a summary of the model.\n",
        "\n",
        "Notice that each layer gets one line in the model summary. As mentioned previously, if you have a large model, this compactness makes it much easier to read the model.\n",
        "\n",
        "For the sake of clarity, let's try creating the mode layers with more than one line of code. This will require us to import some modules from Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWObROuIguEE"
      },
      "source": [
        "# Create first network with Keras\n",
        "\n",
        "#model = tf.keras.models.Sequential()\n",
        "#model.add(tf.keras.layers.Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "#model.add(tf.keras.layers.Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "#model.add(tf.keras.layers.Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "#checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, verbose=1, save_weights_only=True,\n",
        "#                                               monitor=\"val_accuracy\", save_best_only=True, mode=\"max\")\n",
        "\n",
        "# Need to fix!!!\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "\n",
        "# create model - need to fix!!!\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(12, input_dim=8, kernel_initializer='uniform'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(tf.keras.layers.Dense(8, kernel_initializer='uniform'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(tf.keras.layers.Dense(1, kernel_initializer='uniform'))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVTVr8beguEE"
      },
      "source": [
        "Note that by defining the activation function with a different line of code. This allows you to separate the layers with activation functions, which can help clarity. The final decision is up to you, but this gives you something to think about when defining the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPk9RqR1guEE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFiYILANguEE"
      },
      "source": [
        "## Dead Neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSc7mg0iguEE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lFDI8fwguEE"
      },
      "source": [
        "## Using A GPU\n",
        "\n",
        "From: \"https://keras.io/getting-started/faq/#how-can-i-run-keras-on-gpu\"\n",
        "“If you are running on the TensorFlow or CNTK backends, your code will automatically run on GPU\n",
        "if any available GPU is detected.”\n",
        "\n",
        "\"https://kawahara.ca/select-single-gpu-keras/\"\n",
        "\n",
        "https://medium.com/@ab9.bhatia/set-up-gpu-accelerated-tensorflow-keras-on-windows-10-with-anaconda-e71bfa9506d1\n",
        "\n",
        "To run with a CPU only:\n",
        "https://stackoverflow.com/questions/40690598/can-keras-with-tensorflow-backend-be-forced-to-use-cpu-or-gpu-at-will\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "-or-\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS1-qdN8guEE"
      },
      "source": [
        "import os\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.python.client import device_lib\n",
        "import time\n",
        "\n",
        "# The GPU id to use, usually either \"0\" or \"1\";\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"; \n",
        "# CPU only\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"; \n",
        "\n",
        "#print(\"Available GPUs = \",tensorflow_backend._get_available_gpus() )\n",
        "print(\"local devices = \",device_lib.list_local_devices() )\n",
        "print(\" \")\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/config/list_physical_devices\n",
        "print(\"list_physical_devices\", tf.config.list_physical_devices(device_type=None) )\n",
        "print(\" \")\n",
        "\n",
        "# https://www.tensorflow.org/guide/gpu\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "# https://www.tensorflow.org/guide/gpu\n",
        "#tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "try:\n",
        "    # Specify an invalid GPU device\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "        b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "        c = tf.matmul(a, b)\n",
        "    # end with\n",
        "except RuntimeError as e:\n",
        "    print(e)\n",
        "# end try/except\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tch6VTRQguEE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}